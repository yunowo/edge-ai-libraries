# SPDX-FileCopyrightText: (C) 2025 Intel Corporation
# SPDX-License-Identifier: LicenseRef-Intel
---
global:
  huggingface:
    apiToken: <HUGGINGFACEHUB_API_TOKEN>  # Set this during installation
  proxy:
    no_proxy: ""
    http_proxy: ""
    https_proxy: ""
  UI_NODEPORT: <ui-nodeport>
  POSTGRES_USER: <POSTGRES_USER>
  POSTGRES_PASSWORD: <POSTGRES_PASSWORD>
  MINIO_ROOT_USER: <MINIO_ROOT_USER>
  MINIO_ROOT_PASSWORD: <MINIO_ROOT_PASSWORD>
  LLM_MODEL: <LLM_MODEL>
  EMBEDDING_MODEL_NAME: <EMBEDDING_MODEL_NAME>
  RERANKER_MODEL: <RERANKER_MODEL>
  OTLP_ENDPOINT:
  OTLP_ENDPOINT_TRACE:
  teiEmbeddingService:
    enabled: false
  ovmsEmbeddingService:
    enabled: true 
  minio_pvc:
    size: 20Gi
  reranker_pvc:
    size: 20Gi
  tgi_pvc:
    size: 20Gi
  ovms_embed_pvc:
    size: 20Gi
  keeppvc: false #true to persist downloaded models across multiple deployments

Chatqna:
  name: chatqna-backend
  image:
    repository: intel/chatqna
    tag: "1.2.1"
    pullPolicy: IfNotPresent
  service:
    type: ClusterIP
    port: 8100
    targetPort: 8080
  env:
    ENDPOINT_URL: http://text-generation-service
    INDEX_NAME: intel-rag
    FETCH_K: 10
    PORT_DB: 5432/langchain
    SERVICE_NAME: chatqna
    SERVICE_ENV: chatqna
    OTEL_METRICS_EXPORTER: otlp
    OTEL_TRACES_EXPORTER: otlp
    REQUESTS_CA_BUNDLE: /etc/ssl/certs/ca-certificates.crt
    PG_CONNECTION_STRING: postgresql+psycopg://

chatqna-ui:
  nameOverride: "ui"

dataprepPgvector:
  name: document-ingestion
  service:
    type: ClusterIP
    port: 8000
    targetPort: 8000

tgiService:
  name: text-generation-service
  enabled: true
  service:
    port: 8080
vllmService:
  name: vllm-service
  enabled: false
  service:
    port: 8080
ovmsService:
  name: ovmsService
  enabled: false
  service:
    port: 8300
reranker:
  service:
    port: 8090
chatqnaui:
  service:
    port: 80

nginxService:
  annotations: {}